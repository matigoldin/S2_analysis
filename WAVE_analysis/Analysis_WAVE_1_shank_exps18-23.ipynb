{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File to save all the waveforms in waveforms pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    " %pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "from scipy import *\n",
    "from scipy import stats, io\n",
    "import numpy as np\n",
    "import struct\n",
    "import tables as tb\n",
    "from attrdict import AttrDict\n",
    "import matplotlib.pyplot as plt\n",
    "import os as os\n",
    "from phy.io import KwikModel\n",
    "import codecs as codecs\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.ticker as mtick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------\n",
    "#SAVING BINARY OBJECTS DATA\n",
    "#need to automate data folder creation\n",
    "#----------------------------------------------------------------------------------------\n",
    "import pickle \n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open('data/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('data/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS TO GET WAVE PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------\n",
    "# READKWIKINFO\n",
    "#----------------------------------------------------------------------------------------\n",
    "# We read the data of the output from klusterkwik: spike times and cluster-number of each\n",
    "# cluster-number is in klustaviewa series (can be as high as 130 e.g.)\n",
    "# Grupete stands for cluster groups! 2: good clusters, 1: multiunits, 0: unsorted, 3: noise\n",
    "def readkwikinfo(kwik, grupete=3):\n",
    "    model = KwikModel(kwik) # load kwik model from file\n",
    "    spiketimes = model.spike_times # extract the absolute spike times\n",
    "    clusters = model.cluster_groups # extract the cluster names\n",
    "    sample_rate = model.sample_rate # extract sampling freq\n",
    "    \n",
    "    spikedata = {} # initialise dictionary\n",
    "    for cluster in clusters.keys():\n",
    "        clustergroup = clusters[cluster]\n",
    "        if clustergroup==grupete: # only look at specified type of cluster, 0 = noise, 1 = MUA, 2 = GOOD, 3 = unsorted\n",
    "            spiketimematrix = AttrDict({'spike_times': np.zeros(len(spiketimes[where(model.spike_clusters==cluster)]))})\n",
    "            spiketimematrix.spike_times = spiketimes[where(model.spike_clusters==cluster)]\n",
    "            spikedata[cluster] = spiketimematrix # data structure is a dictionary with attribute accessible spiketimes\n",
    "            # attribute accessible means that spikedata.spike_times works, normal dictionaries would be spikedata[spike_times]\n",
    "    \n",
    "    model.close()\n",
    "    return spikedata, sample_rate\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "# READ KWIK WAVE INFO\n",
    "def readkwikwaveinfo(wavedata,kwik,expe,meas, grupete=2,samples=48,electrodes=8):\n",
    "    model = KwikModel(kwik) # load kwik model from file\n",
    "    spiketimes = model.spike_times # extract the absolute spike times\n",
    "    clusters = model.cluster_groups # extract the cluster names\n",
    "    #sample_rate = model.sample_rate # extract sampling freq\n",
    "    waveforms = model.waveforms\n",
    "    \n",
    "    if wavedata == False: wavedata = dict()\n",
    "    \n",
    "    for cluster in clusters.keys():\n",
    "        clustergroup = clusters[cluster]\n",
    "        if clustergroup==grupete: # only look at specified type of cluster, 0 = noise, 1 = MUA, 2 = GOOD, 3 = unsorted\n",
    "            \n",
    "            wavematrix = AttrDict({'waves': np.zeros(len(spiketimes[where(model.spike_clusters==cluster)]))})\n",
    "            wavematrix.update({'spike_times': np.zeros(len(spiketimes[where(model.spike_clusters==cluster)]))})\n",
    "                        \n",
    "            wavematrix.spike_times = spiketimes[where(model.spike_clusters==cluster)]\n",
    "            \n",
    "            wavematrix.waves = waveforms[(model.spike_clusters==cluster)]\n",
    "                        \n",
    "            #wavematrix.update(AttrDict({'meanwave' : np.zeros([samples,electrodes]), 'stdwave' : np.zeros([samples,electrodes])}))\n",
    "            #wavematrix.meanwave = mean(wavematrix.waves[:,:,:],axis=0)\n",
    "            #wavematrix.stdwave = std(wavematrix.waves[:,:,:],axis=0)\n",
    "            \n",
    "            #parnames, params, electrode,bigwave = Getwaveparams(wavematrix.meanwave)\n",
    "            \n",
    "            #wavematrix.update(AttrDict({'params' : params, 'electrodemax': electrode,'parnames': parnames, 'bigwave' : bigwave}))\n",
    "            \n",
    "            wavematrix.update(AttrDict({'exp' : int(expe) , 'meas': int(meas[1]) , 'shank': int(meas[3]), 'clusnum': int(cluster)}))\n",
    "                                        #, 'bigwavestd': wavematrix.stdwave[:,electrode]  }))\n",
    "            codename = 'exp'+ str(expe) + '_' +meas + '_c' + str(cluster)\n",
    "            \n",
    "            #Comment to save all waveforms We don't save now for just only having the features\n",
    "            #wavematrix.waves = []\n",
    "                        \n",
    "            wavedata[codename] = wavematrix\n",
    "            # attribute accessible means that spikedata.spike_times works, normal dictionaries would be spikedata[spike_times]\n",
    "    \n",
    "    \n",
    "    model.close()\n",
    "    return \n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "#GET WAVE PARAMS\n",
    "def Getwaveparams(Waves, zero_correct=False):\n",
    "    \n",
    "    #zero correction for putting first sample at 0\n",
    "    if zero_correct: Waves = Waves - Waves[0,:]\n",
    "    \n",
    "    #here I get the minimum of all electrodes\n",
    "    \n",
    "    #here I get the peak to peak maximum\n",
    "    p2p = amax(Waves,0)-amin(Waves,0)\n",
    "       \n",
    "    #here I get the electrode number of biggest waveform\n",
    "    electrode = where(p2p==max(p2p))[0][0]\n",
    "    \n",
    "    #I use the biggest waveform, already corrected for 0\n",
    "    Wave = Waves[:,electrode]\n",
    "    \n",
    "    #I get the minimum\n",
    "    minimo = min(Wave)\n",
    "    \n",
    "    plot(Wave)\n",
    "    plot(arange(48),0*arange(48))\n",
    "\n",
    "    \n",
    "    #I find max and min \n",
    "    minim = where(Wave== minimo)[0][0]\n",
    "    table_max = where(numpy.r_[False, Wave[1:] > Wave[:-1]] & numpy.r_[Wave[:-1] > Wave[1:], False])[0]\n",
    "\n",
    "    #have to get local max before minimum and after minimum\n",
    "    tablebefore = table_max[where(table_max<minim)]\n",
    "    tableafter = table_max[where(table_max>minim)]\n",
    "    \n",
    "    #--------------------------------------------------------------------    \n",
    "    if len(tablebefore)<1:\n",
    "        peak1 = 0   #if there is no second peak, i select the las point\n",
    "    else:\n",
    "        #peak2 = table_max[1] \n",
    "        peak1 =  table_max[Wave[table_max] == max(Wave[tablebefore])]\n",
    "    peak1val = Wave[peak1]/minimo\n",
    "    #--------------------------------------------------------------------\n",
    "    if len(tableafter)<1:\n",
    "        peak2 = 47   #if there is no second peak, i select the las point\n",
    "    else:\n",
    "        #peak2 = table_max[1] \n",
    "        peak2 =  table_max[Wave[table_max] == max(Wave[tableafter])]\n",
    "    peak2val = Wave[peak2]/minimo\n",
    "    #--------------------------------------------------------------------\n",
    "    \n",
    "    #relative to minimum\n",
    "    prel1 = peak1 - minim\n",
    "    prel2 = peak2 -minim\n",
    "            \n",
    "    #I find previous index of 0 crossings\n",
    "    table_0d = where(numpy.r_[Wave[1:] <= 0,False] & numpy.r_[Wave[:-1]>0,False])[0]\n",
    "    table_0u = where(numpy.r_[Wave[1:] >= 0,False] & numpy.r_[Wave[:-1]<0,False])[0]\n",
    "    \n",
    "    c1 = table_0d[0]\n",
    "    c2 = table_0u[0]\n",
    "    \n",
    "    if c2<minim:#if there is an extra crossing\n",
    "        #c1 = table_0d[1]\n",
    "        c2 = table_0u[1]\n",
    "    \n",
    "    \n",
    "    #get 0 crossings\n",
    "    cr1 = c1+abs(Wave[c1])/(abs(Wave[c1])+abs(Wave[c1+1]))\n",
    "    cr2 = c2+abs(Wave[c2])/(abs(Wave[c2])+abs(Wave[c2+1]))\n",
    "    #relative to minimum\n",
    "    crel1 = cr1-minim\n",
    "    crel2 = cr2-minim\n",
    "    \n",
    "    #Get the widths\n",
    "    width0 = cr2-cr1\n",
    "    \n",
    "    amp50 = minimo/2\n",
    "    amp25 = minimo/4\n",
    "    \n",
    "    #get the amplitude crossings\n",
    "    table_0d = where(numpy.r_[Wave[1:] <= amp50,False] & numpy.r_[Wave[:-1]>amp50,False])[0]\n",
    "    table_0u = where(numpy.r_[Wave[1:] >= amp50,False] & numpy.r_[Wave[:-1]<amp50,False])[0]\n",
    "        \n",
    "    c1 = table_0d[0]\n",
    "    c2 = table_0u[0]\n",
    "    \n",
    "    cross1 = c1+abs(Wave[c1])/(abs(Wave[c1])+abs(Wave[c1+1]))\n",
    "    cross2 = c2+abs(Wave[c2])/(abs(Wave[c2])+abs(Wave[c2+1]))\n",
    "    \n",
    "    width50 = cross2-cross1\n",
    "    #---------------------------------\n",
    "    \n",
    "    table_0d = where(numpy.r_[Wave[1:] <= amp25,False] & numpy.r_[Wave[:-1]>amp25,False])[0]\n",
    "    table_0u = where(numpy.r_[Wave[1:] >= amp25,False] & numpy.r_[Wave[:-1]<amp25,False])[0]\n",
    "    \n",
    "    c1 = table_0d[0]\n",
    "    c2 = table_0u[0]\n",
    "    \n",
    "    cross1 = c1+abs(Wave[c1])/(abs(Wave[c1])+abs(Wave[c1+1]))\n",
    "    cross2 = c2+abs(Wave[c2])/(abs(Wave[c2])+abs(Wave[c2+1]))\n",
    "    \n",
    "    width25 = cross2-cross1\n",
    "    \n",
    "    pnames =['widths0-25-50','cross1-2','crossrel1-2','peakVals', 'peaks1-2','peaksrel1-2','mins']\n",
    "\n",
    "    \n",
    "    return pnames,[[width0, width25, width50],[cr1, cr2],[crel1,crel2],[peak1val,peak2val], [peak1, peak2],[prel1,prel2],  [minim, minimo]], electrode,Wave\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "#Wave DATABASE\n",
    "#def Wavedatabase(expe,meas,cluster,electrode,params):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Files and Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In this cell you put all the information to make the code portable from computer to computer\n",
    "# You have to place all the file names and experiments, then you loop whichever you want to analyse\n",
    "#--------------------------------------------------------------------------------\n",
    "#Experiment numbers\n",
    "ExpeNum = [20,22,23,24,26,27,28,29,30,31,32]\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "#Folders for measurements and experiments (this is how we separate shanks in folders for individual analyses)\n",
    "m164 = ['m1s1','m1s2','m1s3','m1s4','m1s5','m1s6','m1s7','m1s8']\n",
    "m264 = ['m2s1','m2s2','m2s3','m2s4','m2s5','m2s6','m2s7','m2s8']\n",
    "m364 = ['m3s1','m3s2','m3s3','m3s4','m3s5','m3s6','m3s7','m3s8']\n",
    "m464 = ['m4s1','m4s2','m4s3','m4s4','m4s5','m4s6','m4s7','m4s8']\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "#Kwik files    \n",
    "\n",
    "files20 = [ 'MEAS-150707-1_ele01_ele08.kwik',\n",
    "            'MEAS-150707-1_ele09_ele16.kwik',\n",
    "            'MEAS-150707-1_ele17_ele24.kwik',\n",
    "            'MEAS-150707-1_ele25_ele32.kwik',\n",
    "            'MEAS-150707-23_ele01_ele08.kwik',\n",
    "            'MEAS-150707-23_ele16_ele09.kwik']#,\n",
    "            #'MEAS-150707-23_ele17_ele24.kwik',  not in S2\n",
    "            #'MEAS-150707-23_ele25_ele32.kwik',] not in S2\n",
    "\n",
    "files22 = [ 'MEAS-150716-12_ele01_ele08.kwik',\n",
    "            'MEAS-150716-12_ele09_ele16.kwik',\n",
    "            'MEAS-150716-12_ele17_ele24.kwik',\n",
    "            'MEAS-150716-12_ele25_ele32.kwik',\n",
    "            'MEAS-150716-3_ele01_ele08.kwik',\n",
    "            'MEAS-150716-3_ele09_ele16.kwik',\n",
    "            'MEAS-150716-3_ele17_ele24.kwik',\n",
    "            'MEAS-150716-3_ele25_ele32.kwik',]\n",
    "\n",
    "files23 = [ 'MEAS-151027-1_ele01_ele08.kwik',\n",
    "            'MEAS-151027-1_ele09_ele16.kwik',\n",
    "            #'MEAS-151027-1_ele17_ele24.kwik', out of S2\n",
    "            #'MEAS-151027-1_ele25_ele32.kwik', out of S2\n",
    "            #'MEAS-151027-1_ele33_ele40.kwik', out of S2\n",
    "            #'MEAS-151027-1_ele41_ele48.kwik', out of S2\n",
    "            #'MEAS-151027-1_ele49_ele56.kwik', out of S2\n",
    "            #'MEAS-151027-1_ele57_ele64.kwik', out of S2\n",
    "            'MEAS-151027-2_ele01_ele08.kwik',\n",
    "            'MEAS-151027-2_ele09_ele16.kwik',\n",
    "            'MEAS-151027-2_ele17_ele24.kwik',\n",
    "            'MEAS-151027-2_ele25_ele32.kwik',\n",
    "            'MEAS-151027-2_ele33_ele40.kwik']#,\n",
    "            #'MEAS-151027-2_ele41_ele48.kwik',  out of S2\n",
    "            #'MEAS-151027-2_ele49_ele56.kwik',  out of S2\n",
    "            #'MEAS-151027-2_ele57_ele64.kwik']  out of S2\n",
    "\n",
    "files24 = [#'MEAS-151103-1_EXTRACTED_ele25_ele32.kwik',  no functional responses\n",
    "           'MEAS-151103-1_EXTRACTED_ele33_ele40.kwik',  \n",
    "           'MEAS-151103-1_EXTRACTED_ele41_ele48.kwik',  \n",
    "           'MEAS-151103-1_EXTRACTED_ele49_ele56.kwik',  \n",
    "           'MEAS-151103-1_EXTRACTED_ele57_ele64.kwik',\n",
    "           #'MEAS-151103-2_ele33_ele40.kwik',   no units\n",
    "           'MEAS-151103-2_ele41_ele48.kwik',\n",
    "           'MEAS-151103-2_ele49_ele56.kwik',\n",
    "           'MEAS-151103-2_ele57_ele64.kwik']\n",
    "\n",
    "#OUT OF S2\n",
    "#files25 = [ 'MEAS-151105-1good_ele01_ele08.kwik',\n",
    "#            'MEAS-151105-1good_ele09_ele16.kwik',\n",
    "#            'MEAS-151105-1good_ele17_ele24.kwik',\n",
    "#            'MEAS-151105-1good_ele25_ele32.kwik',\n",
    "#            'MEAS-151105-1good_ele33_ele40.kwik',\n",
    "#            'MEAS-151105-1good_ele41_ele48.kwik',\n",
    "#            'MEAS-151105-1good_ele49_ele56.kwik',\n",
    "#            'MEAS-151105-1good_ele57_ele64.kwik',\n",
    "#            'MEAS-151105-2_ele01_ele08.kwik',\n",
    "#            'MEAS-151105-2_ele09_ele16.kwik',\n",
    "#            'MEAS-151105-2_ele17_ele24.kwik',\n",
    "#            'MEAS-151105-2_ele25_ele32.kwik',\n",
    "#            'MEAS-151105-2_ele33_ele40.kwik',\n",
    "#            'MEAS-151105-2_ele41_ele48.kwik',\n",
    "#            'MEAS-151105-2_ele49_ele56.kwik',\n",
    "#            'MEAS-151105-2_ele57_ele64.kwik']\n",
    "\n",
    "files26 = [ 'MEAS-151110-1_ele01_ele08.kwik',\n",
    "            'MEAS-151110-1_ele09_ele16.kwik',\n",
    "            'MEAS-151110-1_ele17_ele24.kwik',\n",
    "            'MEAS-151110-1_ele25_ele32.kwik',\n",
    "            'MEAS-151110-1_ele33_ele40.kwik',\n",
    "            'MEAS-151110-1_ele41_ele48.kwik',\n",
    "            'MEAS-151110-1_ele49_ele56.kwik',\n",
    "            #'MEAS-151110-1_ele57_ele64.kwik', out of S2\n",
    "            'MEAS-151110-2_ele01_ele08.kwik',\n",
    "            'MEAS-151110-2_ele09_ele16.kwik',\n",
    "            'MEAS-151110-2_ele17_ele24.kwik',\n",
    "            'MEAS-151110-2_ele25_ele32.kwik']#,\n",
    "            #'MEAS-151110-2_ele33_ele40.kwik',  no units\n",
    "            #'MEAS-151110-2_ele41_ele48.kwik', out of S2\n",
    "            #'MEAS-151110-2_ele49_ele56.kwik', no units\n",
    "            #'MEAS-151110-2_ele57_ele64.kwik', no units\n",
    "            #'MEAS-151110-3_ele01_ele08.kwik',  out of S2\n",
    "            #'MEAS-151110-3_ele09_ele16.kwik',  out of S2\n",
    "            #'MEAS-151110-3_ele17_ele24.kwik',  out of S2\n",
    "            #'MEAS-151110-3_ele25_ele32.kwik',  out of S2\n",
    "            #'MEAS-151110-3_ele33_ele40.kwik',  out of S2\n",
    "            #'MEAS-151110-3_ele41_ele48.kwik',  out of S2\n",
    "            #'MEAS-151110-3_ele49_ele56.kwik',  no units\n",
    "            #'MEAS-151110-3_ele57_ele64.kwik']  no units\n",
    "\n",
    "files27  = ['MEAS-151112-1_ele01_ele08.kwik',\n",
    "            'MEAS-151112-1_ele09_ele16.kwik',\n",
    "            'MEAS-151112-1_ele17_ele24.kwik',\n",
    "            'MEAS-151112-1_ele25_ele32.kwik',\n",
    "            'MEAS-151112-1_ele33_ele40.kwik',\n",
    "            'MEAS-151112-1_ele41_ele48.kwik',\n",
    "            'MEAS-151112-1_ele49_ele56.kwik',\n",
    "            'MEAS-151112-1_ele57_ele64.kwik',\n",
    "            'MEAS-151112-2_ele01_ele08.kwik',\n",
    "            'MEAS-151112-2_ele09_ele16.kwik',\n",
    "            'MEAS-151112-2_ele17_ele24.kwik',\n",
    "            'MEAS-151112-2_ele25_ele32.kwik',\n",
    "            'MEAS-151112-2_ele33_ele40.kwik',\n",
    "            'MEAS-151112-2_ele41_ele48.kwik',\n",
    "            'MEAS-151112-2_ele49_ele56.kwik',\n",
    "            'MEAS-151112-2_ele57_ele64.kwik',\n",
    "            'MEAS-151112-3_ele01_ele08.kwik',\n",
    "            'MEAS-151112-3_ele09_ele16.kwik',\n",
    "            'MEAS-151112-3_ele17_ele24.kwik',\n",
    "            'MEAS-151112-3_ele25_ele32.kwik',\n",
    "            'MEAS-151112-3_ele33_ele40.kwik',\n",
    "            'MEAS-151112-3_ele41_ele48.kwik',\n",
    "            'MEAS-151112-3_ele49_ele56.kwik',\n",
    "            'MEAS-151112-3_ele57_ele64.kwik']\n",
    "\n",
    "files28 =  ['MEAS-151116-1_ele01_ele08.kwik',\n",
    "            'MEAS-151116-1_ele09_ele16.kwik',\n",
    "            'MEAS-151116-1_ele17_ele24.kwik',\n",
    "            'MEAS-151116-1_ele25_ele32.kwik',\n",
    "            'MEAS-151116-1_ele33_ele40.kwik',\n",
    "            'MEAS-151116-1_ele41_ele48.kwik',\n",
    "            'MEAS-151116-1_ele49_ele56.kwik',\n",
    "            #'MEAS-151116-1_ele57_ele64.kwik',\n",
    "            'MEAS-151116-2_ele01_ele08.kwik',\n",
    "            'MEAS-151116-2_ele09_ele16.kwik',\n",
    "            'MEAS-151116-2_ele17_ele24.kwik',\n",
    "            'MEAS-151116-2_ele25_ele32.kwik',\n",
    "            'MEAS-151116-2_ele33_ele40.kwik',\n",
    "            'MEAS-151116-2_ele41_ele48.kwik',\n",
    "            'MEAS-151116-2_ele49_ele56.kwik',\n",
    "            'MEAS-151116-2_ele57_ele64.kwik',\n",
    "            'MEAS-151116-3_ele01_ele08.kwik',\n",
    "            'MEAS-151116-3_ele09_ele16.kwik',\n",
    "            'MEAS-151116-3_ele17_ele24.kwik',\n",
    "            'MEAS-151116-3_ele25_ele32.kwik',\n",
    "            'MEAS-151116-3_ele33_ele40.kwik',\n",
    "            'MEAS-151116-3_ele41_ele48.kwik',\n",
    "            'MEAS-151116-3_ele49_ele56.kwik',\n",
    "            'MEAS-151116-3_ele57_ele64.kwik']\n",
    "\n",
    "files29  = ['MEAS-151118-1_ele01_ele08.kwik',\n",
    "            'MEAS-151118-1_ele09_ele16.kwik',\n",
    "            'MEAS-151118-1_ele17_ele24.kwik',\n",
    "            'MEAS-151118-1_ele25_ele32.kwik',\n",
    "            'MEAS-151118-1_ele33_ele40.kwik',\n",
    "            'MEAS-151118-1_ele41_ele48.kwik',\n",
    "            'MEAS-151118-1_ele49_ele56.kwik',\n",
    "            'MEAS-151118-1_ele57_ele64.kwik',\n",
    "            'MEAS-151118-2_ele01_ele08.kwik',\n",
    "            'MEAS-151118-2_ele09_ele16.kwik',\n",
    "            'MEAS-151118-2_ele17_ele24.kwik',\n",
    "            'MEAS-151118-2_ele25_ele32.kwik',\n",
    "            'MEAS-151118-2_ele33_ele40.kwik',\n",
    "            'MEAS-151118-2_ele41_ele48.kwik',\n",
    "            'MEAS-151118-2_ele49_ele56.kwik',\n",
    "            'MEAS-151118-2_ele57_ele64.kwik',\n",
    "            'MEAS-151118-3_ele01_ele08.kwik',\n",
    "            'MEAS-151118-3_ele09_ele16.kwik',\n",
    "            'MEAS-151118-3_ele17_ele24.kwik',\n",
    "            'MEAS-151118-3_ele25_ele32.kwik',\n",
    "            'MEAS-151118-3_ele33_ele40.kwik',\n",
    "            'MEAS-151118-3_ele41_ele48.kwik',\n",
    "            'MEAS-151118-3_ele49_ele56.kwik',\n",
    "            'MEAS-151118-3_ele57_ele64.kwik']\n",
    "\n",
    "\n",
    "files30  = ['MEAS-151208-2_ele01_ele08.kwik',\n",
    "            'MEAS-151208-2_ele09_ele16.kwik',\n",
    "            'MEAS-151208-2_ele17_ele24.kwik',\n",
    "            'MEAS-151208-2_ele25_ele32.kwik',\n",
    "            'MEAS-151208-2_ele33_ele40.kwik',\n",
    "            'MEAS-151208-2_ele41_ele48.kwik',\n",
    "            'MEAS-151208-2_ele49_ele56.kwik',\n",
    "            'MEAS-151208-2_ele57_ele64.kwik',\n",
    "            'MEAS-151208-3_ele01_ele08.kwik',\n",
    "            'MEAS-151208-3_ele09_ele16.kwik',\n",
    "            'MEAS-151208-3_ele17_ele24.kwik',\n",
    "            'MEAS-151208-3_ele25_ele32.kwik',\n",
    "            'MEAS-151208-3_ele33_ele40.kwik',\n",
    "            'MEAS-151208-3_ele41_ele48.kwik',\n",
    "            'MEAS-151208-3_ele49_ele56.kwik',\n",
    "            'MEAS-151208-3_ele57_ele64.kwik',\n",
    "            'MEAS-151208-4_ele01_ele08.kwik',\n",
    "            'MEAS-151208-4_ele09_ele16.kwik',\n",
    "            'MEAS-151208-4_ele17_ele24.kwik',\n",
    "            'MEAS-151208-4_ele25_ele32.kwik',\n",
    "            'MEAS-151208-4_ele33_ele40.kwik',\n",
    "            'MEAS-151208-4_ele41_ele48.kwik',\n",
    "            'MEAS-151208-4_ele49_ele56.kwik',\n",
    "            'MEAS-151208-4_ele57_ele64.kwik',\n",
    "            'MEAS-151208-5_ele01_ele08.kwik',\n",
    "            'MEAS-151208-5_ele09_ele16.kwik',\n",
    "            'MEAS-151208-5_ele17_ele24.kwik',\n",
    "            'MEAS-151208-5_ele25_ele32.kwik',\n",
    "            'MEAS-151208-5_ele33_ele40.kwik',\n",
    "            'MEAS-151208-5_ele41_ele48.kwik',\n",
    "            'MEAS-151208-5_ele49_ele56.kwik',\n",
    "            'MEAS-151208-5_ele57_ele64.kwik']\n",
    "\n",
    "\n",
    "files31 = [ #'MEAS-151210-1_ele01_ele08.kwik',  no units\n",
    "            #'MEAS-151210-1_ele09_ele16.kwik',  no units\n",
    "            #'MEAS-151210-1_ele17_ele24.kwik',  no units\n",
    "            #'MEAS-151210-1_ele25_ele32.kwik',  no units\n",
    "            #'MEAS-151210-1_ele33_ele40.kwik',  no units\n",
    "            'MEAS-151210-1_ele41_ele48.kwik',\n",
    "            'MEAS-151210-1_ele49_ele56.kwik',\n",
    "            'MEAS-151210-1_ele57_ele64.kwik',\n",
    "            #'MEAS-151210-2_ele01_ele08.kwik',   out of S2\n",
    "            #'MEAS-151210-2_ele09_ele16.kwik',   out of S2\n",
    "            #'MEAS-151210-2_ele17_ele24.kwik',   out of S2\n",
    "            #'MEAS-151210-2_ele25_ele32.kwik',   out of S2\n",
    "            'MEAS-151210-2_ele33_ele40.kwik',\n",
    "            'MEAS-151210-2_ele41_ele48.kwik',\n",
    "            'MEAS-151210-2_ele49_ele56.kwik',\n",
    "            'MEAS-151210-2_ele57_ele64.kwik',\n",
    "            #'MEAS-151210-3_ele01_ele08.kwik',   out of S2\n",
    "            #'MEAS-151210-3_ele09_ele16.kwik',   no units\n",
    "            #'MEAS-151210-3_ele17_ele24.kwik',   no units\n",
    "            #'MEAS-151210-3_ele25_ele32.kwik',   out of S2\n",
    "            'MEAS-151210-3_ele33_ele40.kwik',\n",
    "            'MEAS-151210-3_ele41_ele48.kwik',\n",
    "            'MEAS-151210-3_ele49_ele56.kwik',\n",
    "            'MEAS-151210-3_ele57_ele64.kwik']\n",
    "\n",
    "files32 = [ 'MEAS-151214-1_ele01_ele08.kwik',\n",
    "            'MEAS-151214-1_ele09_ele16.kwik',\n",
    "            'MEAS-151214-1_ele17_ele24.kwik',\n",
    "            'MEAS-151214-1_ele25_ele32.kwik',\n",
    "            'MEAS-151214-1_ele33_ele40.kwik',\n",
    "            'MEAS-151214-2_ele01_ele08.kwik',\n",
    "            'MEAS-151214-2_ele09_ele16.kwik',\n",
    "            'MEAS-151214-2_ele17_ele24.kwik',\n",
    "            'MEAS-151214-2_ele25_ele32.kwik',\n",
    "            'MEAS-151214-2_ele33_ele40.kwik']\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------\n",
    "# Here I create my dictionary of experiments\n",
    "Expe={}\n",
    "Vtags={}\n",
    "Stim={}\n",
    "for num in ExpeNum: \n",
    "    Expe[num] = dict()\n",
    "    Vtags[num] = dict()\n",
    "#---------------------------------------\n",
    "i=0        \n",
    "for meas in np.append(m164[0:4],m364[0:2]):\n",
    "    Expe[20][meas] = files20[i]\n",
    "    i+=1\n",
    "#---------------------------------------\n",
    "i=0 \n",
    "for meas in np.append(m164[0:4],m364[0:4]):\n",
    "    Expe[22][meas] = files22[i]\n",
    "    i+=1\n",
    "#---------------------------------------\n",
    "i=0 \n",
    "for meas in np.append(m164[0:2],m264[0:5]):\n",
    "    Expe[23][meas] = files23[i]\n",
    "    i+=1\n",
    "#---------------------------------------\n",
    "i=0\n",
    "for meas in np.append(m164[4:8],m264[5:8]):    \n",
    "    Expe[24][meas] = files24[i]\n",
    "    i+=1\n",
    "#---------------------------------------\n",
    "#i=0 \n",
    "#for meas in np.append(m164,m264):    \n",
    "#    Expe[25][meas] = files25[i]\n",
    "#    i+=1\n",
    "#---------------------------------------\n",
    "i=0\n",
    "for meas in np.append(m164[0:7],m264[0:4]):\n",
    "    Expe[26][meas] = files26[i]\n",
    "    i+=1\n",
    "#---------------------------------------\n",
    "i=0\n",
    "for meas in np.append(np.append(m164[0:7],m264),m364):    \n",
    "    Expe[28][meas] = files28[i]\n",
    "    i+=1\n",
    "#---------------------------------------    \n",
    "i=0\n",
    "for meas in np.append(np.append(m164,m264),m364):    \n",
    "    Expe[27][meas] = files27[i]\n",
    "    Expe[29][meas] = files29[i]\n",
    "    Expe[30][meas] = files30[i]\n",
    "    i+=1\n",
    "#---------------------------------------    \n",
    "for meas in m464:\n",
    "    Expe[30][meas] = files30[i]\n",
    "    i+=1\n",
    "i=0\n",
    "#---------------------------------------    \n",
    "i=0\n",
    "for meas in np.append(np.append(m164[5:8],m264[4:8]),m364[4:8]):    \n",
    "    Expe[31][meas] = files31[i]\n",
    "    i+=1\n",
    "i=0\n",
    "#---------------------------------------    \n",
    "for meas in np.append(m164[0:5],m264[0:5]):\n",
    "    Expe[32][meas] = files32[i]\n",
    "    i+=1\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "#Vtag files \n",
    "Vtags[20] = ['MEAS-150707-1_Vtag1.dat','nada','MEAS-150707-23_Vtag1.dat']\n",
    "Vtags[22] = ['MEAS-150716-12_Vtag1.dat','nada','MEAS-150716-3_Vtag1.dat']\n",
    "Vtags[23] = ['MEAS-151027-1_Vtag1.dat','MEAS-151027-2_Vtag1.dat']\n",
    "Vtags[23] = ['MEAS-151027-1_Vtag1.dat','MEAS-151027-2_Vtag1.dat']\n",
    "Vtags[24] = ['MEAS-151103-1_Vtag1.dat','MEAS-151103-2_Vtag1.dat']\n",
    "#Vtags[25] = ['MEAS-151105-1good_Vtag1.dat','MEAS-151105-2_Vtag1.dat']\n",
    "Vtags[26] = ['MEAS-151110-1_Vtag1.dat','MEAS-151110-2_Vtag1.dat','MEAS-151110-3_Vtag1.dat']\n",
    "Vtags[27] = ['MEAS-151112-1_Vtag1.dat','MEAS-151112-2_Vtag1.dat','MEAS-151112-3_Vtag1.dat']\n",
    "Vtags[28] = ['MEAS-151116-1_Vtag1.dat','MEAS-151116-2_Vtag1.dat','MEAS-151116-3_Vtag1.dat']\n",
    "Vtags[29] = ['MEAS-151118-1_Vtag1.dat','MEAS-151118-2_Vtag1.dat','MEAS-151118-3_Vtag1.dat']\n",
    "Vtags[30] = ['MEAS-151208-2_Vtag1.dat','MEAS-151208-3_Vtag1.dat','MEAS-151208-4_Vtag1.dat','MEAS-151208-5_Vtag1.dat']\n",
    "Vtags[31] = ['MEAS-151210-1_Vtag1.dat','MEAS-151210-2_Vtag1.dat','MEAS-151210-3_Vtag1.dat']\n",
    "Vtags[32] = ['MEAS-151214-1_Vtag1.dat','MEAS-151214-2_Vtag1.dat']\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "#Stimulus type\n",
    "for i in range(23,33):\n",
    "    Stim[i] = 'big_STIM_FC_corrected'\n",
    "for i in range(15,23):\n",
    "    Stim[i] = 'big_STIM'\n",
    "for i in range(10,15):\n",
    "    Stim[i] = 'small_STIM'  \n",
    "    \n",
    "#--------------------------------------------------------------------------------\n",
    "#Root folder to work in, such all will be in subfolders \n",
    "#e.g.: \"/EXP_23/m1s1/\" for data or \"/STIM/\" for stims\n",
    "rootF = '/home/matias/WORKSPACE/'    \n",
    "stimFolder = rootF +'STIM/'\n",
    "\n",
    "#I have a separate folder for exp 22 and before\n",
    "rootF_kwiks = rootF    #uncomment this to work with the other root folder\n",
    "#rootF_kwiks = '/media/matias/DATA/WORKSPACE2/'\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "#Initialize wave features dictionary\n",
    "Wavedata = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and load data files from experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "group: 2\n",
      "m2s1\n",
      "   building waves\n",
      "m2s2\n",
      "   building waves\n",
      "m2s3\n",
      "   building waves\n",
      "m2s4\n",
      "   building waves\n"
     ]
    }
   ],
   "source": [
    "global binname, textname\n",
    "#---------------------------------------------------------------------------------------\n",
    "SelExp = [28]   #Expe                                        #select experiment numbers!\n",
    "grupete = [2]   #select cluster groups! 2 for good clusters 1 for multiunits, 3 for unsorted\n",
    "\n",
    "#select measurement and/or shanks!\n",
    "Measurements = m264[0:4]          #['m1s1']#['m3s1','m3s3']#m12[-4:]#['m1s1','m1s2','m1s3','m1s4']   \n",
    "\n",
    "# choice code not to ploteverything at the same time\n",
    "ploteo = [0,0,0,0]                                           #1 to make plots: psth,sta,ufc,stc\n",
    "\n",
    "dirs =[]\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Loop Experiments\n",
    "#--------------------------------------------------------------------------------\n",
    "last_exp=0     #we use this to load stim only when we change experiment\n",
    "for expe in SelExp:\n",
    "    \n",
    "    #uncomment if we want a whole file for all the experiments\n",
    "    Wavedata = dict()\n",
    "    \n",
    "    #Measurements = sorted(Expe[expe])                         #uncommento to select all\n",
    "    print(expe)\n",
    "\n",
    "    #--------------------------------------------------------------------------------\n",
    "    #loop goodunits\n",
    "    for group in grupete:   #2 for good clusters 1 for multiunits 3 for unsorted\n",
    "        #folder names\n",
    "        if group ==2:\n",
    "            dirs  = [rootF + 'OUTPUT/PDFwaves/EXP_'+str(expe)]\n",
    "        print('group:', group)\n",
    "        #--------------------------------------------------------------------------------\n",
    "        #loop measurements and shanks\n",
    "        measurements = Expe[expe]                            \n",
    "                 \n",
    "        for meas in Measurements:           \n",
    "            print(meas)\n",
    "            #---------------------------------------------------------------\n",
    "            #select datafile\n",
    "            sp_file = rootF_kwiks + 'EXP_' + str(expe) +'/Spike_Sorting/'+ meas +'/'+ measurements[meas]\n",
    "            \n",
    "            #load datafile\n",
    "            print('   building waves')\n",
    "            readkwikwaveinfo(Wavedata,sp_file,str(expe),meas,group)  \n",
    "            \n",
    "            #---------------------------------------------------------------\n",
    "            #load stimulus if looping new experiment, without trimming\n",
    "           \n",
    "            if len(Wavedata.keys())>0:                              #do only if there are clusters\n",
    "                #--------------------------------------------------------------------------------\n",
    "                #create output folders\n",
    "                for dir in dirs:\n",
    "                    if not os.path.exists(dir):\n",
    "                        os.makedirs(dir) \n",
    "                dire = dirs[0] +'/'\n",
    "                titles = 'Exp'+ str(expe) + '_Meas_' + meas[1] + '_Shank_' + meas[3]\n",
    "                #--------------------------------------------------------------------------------\n",
    "                #Wave params\n",
    "                \n",
    "                #print('   building waves')\n",
    "                                        \n",
    "                \n",
    "                #--------------------------------------------------------------------------------\n",
    "                #Plot Waves\n",
    "                #print('   plotting')\n",
    "                if ploteo[0]==1: display_all_PSTHs_of_recording(expe,hist_output, PSTH_spikes_counts, dire, t_before, t_after,group,STC_on,PW,titles)\n",
    "         \n",
    "    filesave ='waveforms' + str(expe) + 'm2_s1-s4'\n",
    "    save_obj(Wavedata,filesave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
